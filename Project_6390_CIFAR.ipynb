{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 6390 CIFAR",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Exemples de machine learning : projet Seedbank\n",
        "\n",
        "Pour voir des exemples de bout en bout des analyses interactives de machine learning rendues possibles par Colaboratory, découvrez le projet <a href=\"https://research.google.com/seedbank/\">Seedbank</a>.\n",
        "\n",
        "Voici quelques exemples :\n",
        "\n",
        "- <a href=\"https://research.google.com/seedbank/seed/neural_style_transfer_with_tfkeras\">Transfert de style neuronal</a> : utiliser le deep learning pour transférer un style d'une image à une autre.\n",
        "- <a href=\"https://research.google.com/seedbank/seed/ez_nsynth\">EZ NSynth</a> : synthétiser des sons avec les auto-encodeurs WaveNet.\n",
        "- <a href=\"https://research.google.com/seedbank/seed/fashion_mnist_with_keras_and_tpus\">Fashion MNIST avec Keras et TPU</a> : classer des images liées à la mode en utilisant le deep learning.\n",
        "- <a href=\"https://research.google.com/seedbank/seed/deepdream\">DeepDream</a> : produire des images DeepDream à partir de vos propres photos.\n",
        "- <a href=\"https://research.google.com/seedbank/seed/convolutional_vae\">Auto-encodeur variationnel convolutif</a> : créer un modèle génératif de chiffres manuscrits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRsNfFquJdxp",
        "colab_type": "code",
        "outputId": "9e0fa7c5-5e29-4809-d2d4-dd5d75a0be36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i2wjkzQJd_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URR_lpWXJeMk",
        "colab_type": "code",
        "outputId": "cfd1d820-ce1e-463d-a2cc-01ab3fa32844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        }
      },
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "true_y_train=y_train.copy()\n",
        "true_y_test=y_test.copy()\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHcZBBCKc6Wv",
        "colab_type": "code",
        "outputId": "125d184a-c12c-4b87-b476-425eb3a77b52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (5,5), padding='same',\n",
        "                 input_shape=x_train.shape[1:],name='conv1'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3), name='conv2'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), name='maxpool1'))\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', name='conv3'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),name='conv4'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2'))\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, name='fcn1'))\n",
        "model.add(Activation('relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, name='fcn2'))\n",
        "model.add(Activation('softmax'))\n",
        "print(model.summary())\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    log=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    log=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (Conv2D)               (None, 32, 32, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "maxpool1 (MaxPooling2D)      (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "maxpool2 (MaxPooling2D)      (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "fcn1 (Dense)                 (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fcn2 (Dense)                 (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,252,394\n",
            "Trainable params: 1,252,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1563/1563 [==============================] - 44s 28ms/step - loss: 1.7410 - acc: 0.3680 - val_loss: 1.4555 - val_acc: 0.4681\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.4716 - acc: 0.4708 - val_loss: 1.2956 - val_acc: 0.5377\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.3380 - acc: 0.5214 - val_loss: 1.2516 - val_acc: 0.5579\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.2398 - acc: 0.5616 - val_loss: 1.1311 - val_acc: 0.5982\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1611 - acc: 0.5895 - val_loss: 1.0506 - val_acc: 0.6304\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1007 - acc: 0.6099 - val_loss: 0.9994 - val_acc: 0.6511\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0492 - acc: 0.6319 - val_loss: 0.9967 - val_acc: 0.6445\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0080 - acc: 0.6496 - val_loss: 0.9313 - val_acc: 0.6691\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9672 - acc: 0.6636 - val_loss: 0.9367 - val_acc: 0.6763\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9298 - acc: 0.6753 - val_loss: 0.9188 - val_acc: 0.6824\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8997 - acc: 0.6863 - val_loss: 0.8654 - val_acc: 0.6972\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8627 - acc: 0.6995 - val_loss: 0.8343 - val_acc: 0.7101\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8385 - acc: 0.7065 - val_loss: 0.8237 - val_acc: 0.7216\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.8083 - acc: 0.7182 - val_loss: 0.7475 - val_acc: 0.7423\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7879 - acc: 0.7251 - val_loss: 0.8341 - val_acc: 0.7162\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7661 - acc: 0.7322 - val_loss: 0.7300 - val_acc: 0.7475\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7463 - acc: 0.7406 - val_loss: 0.7312 - val_acc: 0.7486\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7291 - acc: 0.7478 - val_loss: 0.7271 - val_acc: 0.7490\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7087 - acc: 0.7533 - val_loss: 0.7129 - val_acc: 0.7596\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.6934 - acc: 0.7579 - val_loss: 0.6948 - val_acc: 0.7561\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.6795 - acc: 0.7623 - val_loss: 0.7234 - val_acc: 0.7567\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.6629 - acc: 0.7693 - val_loss: 0.6608 - val_acc: 0.7731\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.6518 - acc: 0.7742 - val_loss: 0.6881 - val_acc: 0.7675\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.6370 - acc: 0.7789 - val_loss: 0.6291 - val_acc: 0.7833\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.6268 - acc: 0.7823 - val_loss: 0.6823 - val_acc: 0.7619\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.6187 - acc: 0.7871 - val_loss: 0.6680 - val_acc: 0.7750\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.6076 - acc: 0.7905 - val_loss: 0.7150 - val_acc: 0.7593\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.5973 - acc: 0.7924 - val_loss: 0.6461 - val_acc: 0.7840\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.5852 - acc: 0.7960 - val_loss: 0.6366 - val_acc: 0.7850\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5833 - acc: 0.7988 - val_loss: 0.6495 - val_acc: 0.7769\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5742 - acc: 0.8035 - val_loss: 0.6515 - val_acc: 0.7817\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5658 - acc: 0.8051 - val_loss: 0.6671 - val_acc: 0.7819\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5614 - acc: 0.8063 - val_loss: 0.6369 - val_acc: 0.7828\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5493 - acc: 0.8105 - val_loss: 0.6272 - val_acc: 0.7876\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5457 - acc: 0.8119 - val_loss: 0.6211 - val_acc: 0.7948\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5400 - acc: 0.8141 - val_loss: 0.6232 - val_acc: 0.7880\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5362 - acc: 0.8149 - val_loss: 0.5891 - val_acc: 0.8055\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5318 - acc: 0.8163 - val_loss: 0.6752 - val_acc: 0.7816\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5281 - acc: 0.8173 - val_loss: 0.7448 - val_acc: 0.7579\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5238 - acc: 0.8211 - val_loss: 0.6096 - val_acc: 0.7953\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5204 - acc: 0.8210 - val_loss: 0.6937 - val_acc: 0.7820\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5138 - acc: 0.8237 - val_loss: 0.5995 - val_acc: 0.7936\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5164 - acc: 0.8233 - val_loss: 0.5808 - val_acc: 0.8122\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5095 - acc: 0.8246 - val_loss: 0.5954 - val_acc: 0.8066\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5115 - acc: 0.8248 - val_loss: 0.6156 - val_acc: 0.7996\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5082 - acc: 0.8259 - val_loss: 0.5993 - val_acc: 0.8024\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5050 - acc: 0.8276 - val_loss: 0.6654 - val_acc: 0.7961\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5040 - acc: 0.8267 - val_loss: 0.5994 - val_acc: 0.7975\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5013 - acc: 0.8271 - val_loss: 0.6060 - val_acc: 0.8041\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4978 - acc: 0.8294 - val_loss: 0.5979 - val_acc: 0.8061\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4933 - acc: 0.8303 - val_loss: 0.6374 - val_acc: 0.8035\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.4963 - acc: 0.8296 - val_loss: 0.5964 - val_acc: 0.8114\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4963 - acc: 0.8324 - val_loss: 0.6137 - val_acc: 0.8035\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.4934 - acc: 0.8322 - val_loss: 0.6073 - val_acc: 0.8023\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 0.4891 - acc: 0.8314 - val_loss: 0.6713 - val_acc: 0.7811\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.4951 - acc: 0.8320 - val_loss: 0.6191 - val_acc: 0.8016\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4950 - acc: 0.8317 - val_loss: 0.6283 - val_acc: 0.7955\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4950 - acc: 0.8319 - val_loss: 0.6032 - val_acc: 0.8048\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4952 - acc: 0.8309 - val_loss: 0.7034 - val_acc: 0.7922\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4920 - acc: 0.8325 - val_loss: 0.6750 - val_acc: 0.7941\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4956 - acc: 0.8324 - val_loss: 0.6263 - val_acc: 0.8079\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4978 - acc: 0.8293 - val_loss: 0.6609 - val_acc: 0.7875\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4982 - acc: 0.8312 - val_loss: 0.6502 - val_acc: 0.7983\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.4974 - acc: 0.8307 - val_loss: 0.6559 - val_acc: 0.7936\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5004 - acc: 0.8294 - val_loss: 0.7251 - val_acc: 0.7746\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5027 - acc: 0.8295 - val_loss: 0.5776 - val_acc: 0.8042\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5031 - acc: 0.8302 - val_loss: 0.6855 - val_acc: 0.7970\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5028 - acc: 0.8308 - val_loss: 0.6236 - val_acc: 0.8050\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5032 - acc: 0.8314 - val_loss: 0.6550 - val_acc: 0.8038\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5053 - acc: 0.8284 - val_loss: 0.6543 - val_acc: 0.7942\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5108 - acc: 0.8279 - val_loss: 0.7102 - val_acc: 0.7938\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5069 - acc: 0.8285 - val_loss: 0.6916 - val_acc: 0.7846\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5038 - acc: 0.8277 - val_loss: 0.6288 - val_acc: 0.7975\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5025 - acc: 0.8284 - val_loss: 0.7331 - val_acc: 0.7764\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5027 - acc: 0.8287 - val_loss: 0.6049 - val_acc: 0.8041\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.5131 - acc: 0.8264 - val_loss: 0.6532 - val_acc: 0.7864\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5048 - acc: 0.8269 - val_loss: 0.6642 - val_acc: 0.7834\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5080 - acc: 0.8279 - val_loss: 0.6895 - val_acc: 0.7935\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5094 - acc: 0.8263 - val_loss: 0.6106 - val_acc: 0.7937\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5090 - acc: 0.8276 - val_loss: 0.6544 - val_acc: 0.7990\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5166 - acc: 0.8246 - val_loss: 0.6761 - val_acc: 0.7943\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5099 - acc: 0.8283 - val_loss: 0.7361 - val_acc: 0.7885\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5066 - acc: 0.8280 - val_loss: 0.7445 - val_acc: 0.7913\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5124 - acc: 0.8270 - val_loss: 0.6200 - val_acc: 0.8023\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5128 - acc: 0.8268 - val_loss: 0.6354 - val_acc: 0.8080\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5115 - acc: 0.8272 - val_loss: 0.6655 - val_acc: 0.7880\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5110 - acc: 0.8267 - val_loss: 0.6362 - val_acc: 0.7985\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5088 - acc: 0.8269 - val_loss: 0.6341 - val_acc: 0.7939\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5086 - acc: 0.8267 - val_loss: 0.6904 - val_acc: 0.8083\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5050 - acc: 0.8295 - val_loss: 0.6054 - val_acc: 0.8103\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5088 - acc: 0.8284 - val_loss: 0.7960 - val_acc: 0.7774\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5110 - acc: 0.8291 - val_loss: 0.6373 - val_acc: 0.8042\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5085 - acc: 0.8281 - val_loss: 0.6233 - val_acc: 0.8034\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5178 - acc: 0.8258 - val_loss: 0.6780 - val_acc: 0.7929\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5055 - acc: 0.8302 - val_loss: 0.7078 - val_acc: 0.7970\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5102 - acc: 0.8288 - val_loss: 0.6723 - val_acc: 0.7998\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5147 - acc: 0.8269 - val_loss: 0.6301 - val_acc: 0.8085\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5106 - acc: 0.8274 - val_loss: 0.6584 - val_acc: 0.8050\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5084 - acc: 0.8284 - val_loss: 0.6416 - val_acc: 0.8052\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.5089 - acc: 0.8284 - val_loss: 0.7041 - val_acc: 0.8045\n",
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "10000/10000 [==============================] - 1s 74us/step\n",
            "Test loss: 0.7040546877861023\n",
            "Test accuracy: 0.8045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h3lJe7juOnQ",
        "colab_type": "code",
        "outputId": "8687efd9-aaf5-4b59-efa4-89d6d1dbf656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax1=plt.plot(log.history['val_acc'])\n",
        "ax2=plt.plot(log.history['acc'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend((ax1[0],ax2[0]),('Validation accuracy','Training Accuracy'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3hUZdrA4d87k957AgmQUEOooUsR\nEBARBUUQUNeCrAsriu7qLrquXT/W3XWtqyKCuCqIFVCwgCigSAkQSkJPgFTSSG+TvN8fJ4QEAoSQ\nyZDMc19XrmTOnDnznElynvN2pbVGCCGE/TLZOgAhhBC2JYlACCHsnCQCIYSwc5IIhBDCzkkiEEII\nO+dg6wAuVUBAgA4PD7d1GEII0azExMRkaq0D63qu2SWC8PBwtm/fbuswhBCiWVFKHTvfc1I1JIQQ\ndk4SgRBC2DlJBEIIYeckEQghhJ2TRCCEEHZOEoEQQtg5SQRCCGHnmt04AiGaHa3h2C+QuhvMjmBy\nACcP8A4Dnzbg6gdFWVCYASW54OgGTu7g6AqWUrCUQGUFeLUGz1ZgMkFpASRvh+QdUFEGJjOYHCHi\nagjtY91zUcp6xxc2IYlAiMuVnwYn4yEvxfjSFeARBB7BkHkQdnwA2Ucb570cXIzj5p4AXVn3PqH9\nYMB9RnJI3AiJv0BZIbj5gasveLeBkO4Q0gM8WxvxVlZAaT7kp0BeKhSkQWEWFGVCYaaRpAozAQ3d\nb4H+90Lr6MY5p+YkN9lIzL7hxud7WkW58ftwcD7/a1Nj4ciPEBgJrXqDVyurh1tfqrktTNOvXz8t\nI4uF1WkNOQnGHbejq3Hx9GkDKOOCWXIKEjZA3Ao4seXCx2o3BPrcCR3HANq4gy/Jg9wkyD0OxTng\nFgDugeDiDZZiKCuC8mJwcDIu/spk7J991Eg2AZ2gzQDjou/sBZUWKCuA3cth6wLIPmK8t7MXtBts\nHL84B4qzjWMUpF84ZpOjEY+7v/FajyDje0ku7PsCyosguAf4tAVXH6OEU5oHRdnGdzd/o8TjGVKV\nYNKMr+Ic4/nSfAiKMpJK1xuNY1SUG8kmL8X4XE4dB7MTtO5jJC0HF2P7yf3G59HhGjBX3ctWWCB2\nKRxYbVyMnTyMz9KnrXHR9m5jlLIcXIzPtKzIiKGsEJzcjATp4m1czE+XwpTJeH9lgsNrjYSe8LPx\nfmYn8O9ofM9LMRIlGOfsG27E22UctB0MZfnw4wuw/b3ayduzFYQPNUpxYQOgstz4uzj9O8o6AqeO\nGedG1XX6qvshcny9/4xrUkrFaK371fmcJAJhl7Q2LkzlRcY/fVkhZCcYd/AZ+yFp28UvlmBcDKMm\nQNurwDvUuMM2mY0LQ0G6cXHxa2/986mpshKO/2oksJBeZy6WNRWchLQ9xoXXZK6qrnI3Lk6erYzS\nw/mqgEpyIXYZxK8yLvwlp4yqKhcv43XOXsZxc5OMi6AyGwnBI/jMBdfRDY5tgpxE42Lq5GFcAM9H\nmY0LfHnRmW1eYdB/Bni3hZ//AVmHqu7UHY3fZ3GOkVQbi3dbiL7D+D1nHoSMg0YC9mptfJ2+ecg6\nYny2FaVGtZ9SRiz9fw9DHzISXMou428sYQMUnqz7/dz8jfNxcK36DBQM+iNEXt+g8CURCPuUeRh2\nfQjHfjUuTm7+xt1gxkE4GWfcmZ5NmcEvAkL7QttBENbfuFPNPQGnThh3h86e4OwBIT3Bv0PTn1dz\nUlpgXPRNdfRL0RpSdsC+r4wLt0cQuAcYydSnrVECKysy9kmOMX4OioTArkai3fYuHP3JOFZgJFzz\nBETecCaBaW0kpJxE4/d3OulbyoxSgLOXkfxOJ42SU1UJx8VIOrrSKL1VlBm/64jhdZ/H+c77yDqI\n/9ooqY14DFr1rPszyDhgVBs5uhrJ1MXHSACuPg34wM9PEoFo2SorYO/ncPRnjCK0MqpGjm82/rHD\n+ht3Z0VZxsUkoDMEd4PALsbFwMHZ+Cf0DQffCCNZiOYh44Bxh93hmtp19uIcF0oE0lgsmi+tjeqJ\n9S9CRrxRp22uaqxz9YHRT0Ov6Ua1hGiZArsYX+KySCIQV77KSjiVaNS7pu2FzANGY1p2olEHHdAZ\nprwPXSfWv+guhKgmiUBcWSosRp/7A6uNevzcJOOrosx4XpmNKhy/9kaPjLD+0O3muhtEhRD1Iv89\nwrbyUiF1l3HRT9sLR9cbDXcOLkYXvFa9je6Ffh2Mx0Fdjfr8BjiYns/TK/fxpzGd6Rfu18gncq7Y\nE6fIKizlmshgq7+XaBpaa1QLHFAniUDYRvEp+Pkl2PqO0QUPjL7encZC1xuMxj8n90Z9ywUbjvLr\nkSy2JvzG49d35Z4h4Vb7p95xPIfb391CcXkFr07rzcTeoVZ5H3HpLvVifjKvhO/j0tlwMIPNR7KY\n2r8NT9wQ1WjxVFZq4lLziGrlhclkmyQjiUA0nfISo//1sV9hwz+NXjx974Letxvd/1y8rPbW+SXl\nfLM7lRt7taakvIJnv45jx/Ec/jm5F65ODe9torXmaGYh7k4OhHi7AHAoPZ8Z728jyMuZYE8X/rw8\nFm9XR0Z0CWqs0zlHfkk5N7y+iZnD2vO7Qe2s9j7NmdaahRsTeHXdIZbM6E/fdhcvFRaUWrj+tU1k\nFpQS6uOKl6sj6/afbNRE8OzXcbz/ayIdgzy4f2QHbuzZGgdz07Z1WTURKKWuA14FzMBCrfX8s55v\nCywBfKr2mae1Xm3NmEQTyjhgXPRPbDUGz2QfOTOysu1VMO4LaNXrooeZ8/EOfN2ceO6m7g0OZVVs\nKsXlFdw7NIKeod68s+EoL323n4z8Uhbd3R9350v7V/j1cCYrdqWw4VAGqbklxlifCH+u7xHCf386\ngqPZxP9mDMTH3ZFp7/zG7A93sGTGAPqH+15yKeRkfgn/XX+E2SM6EOzlUuc+721K4FhWER9uPtag\nRJB8qpiUU8X0P6vKbG1cOks2JxLm60bnYA/6tvOlZ9jl92+POZZNWz93Aj0vMCVDIyostfCXz3fz\nze5UAD7acrxeiWDJr4lkFpTy4b0DGdLRn3c2HGX+mv3kFJbh636mm/HB9Hy2JWbTytuFEC9X2ge6\n4+J48RuMj7cc5/1fExnXPYSEzEIe/iSWf6w5gI+bI2WWSiyVGl83RwI9XQjycuam3qEMiGj8ak2r\nJQKllBl4ExgDJAHblFIrtdZxNXZ7AliutX5LKRUFrAbCrRWTaCIFJ+Hbx2DvZ8ZjtwBjOoTuk4w7\n/6Aoo8tfPS6Ih0/m8/XuVNyczPxtfNd6/XPV5ZNtx+kS7EmvMG+UUswe0YHWPi78aXksdy7ayuJ7\n+uPmaObngxl8vy+d6QPb0rtN3Re8xb8k8MyqODxdHBjaMYAHrgnkZH4JX+1M5u8r9uHp7MAnf7iK\ntv5uACyZMYDJb//Kre9sxtPFgU5BHoyJCmH2iNqD0SoqNbFJp4hu41OdLMorKpnz8U62JmSTWVDK\nG7edO6FcdmEZCzcm4OXiwIH0fOJT8+ja6tJKV0+t2Mfa+HTmT+rBtAFtAfj1SCZ//GgHfu5O7EnO\nZenWcgDe+V1fxnZreJfcwlIL0xdsYXRUEP+9vW+Dj1NfJ/NLuGPhFg6fLGDeuEgSMgpZtTuF52+y\n4OZ0/ktgfkk57248yjWRQQztFABAdNXfxM4TObXafh77Yg8xx3KqH7f1c2PVA0PxdnU87/E3H8ni\nyRV7Gd45kNenR2NSinX7T/J5TBIVWuPkYMLBpMgpKicpp4idx3OIbuPTvBIBMAA4rLU+CqCUWgZM\nBGomAg2c/ov1BlKsGI+wtsoK2Pkh/PB3Y56c4X+FnlONHj4NrIv/8LfjABSVVfDb0awGVa/Ep+YR\nm5TLkzdE1bobn9g7FCeziQeW7mTSf38lv6Sc9LxSlIIVscm8Oi261gVPa83LPxzk9R8PM7ZbMK9O\ni66VmOaO6sTupFw8XBzoEOhRvT3Q05nPZg1mzd5UDqUXsON4Dv/4dj9jooLoGORZvd/7vyby3Ndx\nTOoTyv9N6oGzg5mXvt3P1oRsBkb48fXuVO4anH3OXfvbPx+hqMzC0t8P4raFW/hqV/IlJYKKSs3W\nhCycHEzM+2IPGugV5sMfPoihnb8bn866Cm9XRzLyS5n5wXYe+TSWqFZetPFzu5RfQ7WtidmUVVTy\n/b50TuaVEHSeUs75FJVZyCkqJ9Tn4p0GtNY8/sVejmUV8cGMgQztFMBvR7P4ZPsJfohLr9V2c3bb\nwZJfEzlVVM5DoztVb+sR5o3ZpNh5/FR1IsgrKWfXiVPcPTicib1bcyi9gMe+3MMzK/fx8tTe58Rk\nqahk/YEMHv0slnb+brx+W3R1VdCYqGDGRDV95wJrVkSFAidqPE6q2lbT08AdSqkkjNLAA3UdSCl1\nn1Jqu1Jqe0ZGhjViFZcjL9Vo+H2lJ6x6EIK7w6xfYOTjxhQMDUwCRWUWPo9JYmy3YNyczKyLP8+c\nLBfxybYTOJlN3Bx9boPtuB6tePuOvpzMK6Fba2/evqMvm+eNIjLEi1kfxrBoUwIH0/P5cmcS93+8\ng9d/PMz0AW347+19zymdKKXo1canVhI4LdDTmTuvCue5m7qz+O7+KGVUV9X01c5kvF0d+WJHMncs\n3MJHW47x7sYE7rqqHYvv6U+IlwvProqjsvLMbABpuSUs+TWRm6PDGNjen2GdAli1K6V6n4pKzcwl\n25n9YQxpuSV1fj4H0vLJK7Hw3MRujOwSyGNf7GHags14uDiwZMYAfNycUEoR5OXCm1Ulkvs/3kGp\npeKCn7ulopI31x8mNbf2fD+bj2ThYFJYKjXLtp04z6vrtvN4DmNf2cDwl9bzr+8OUFJ+4RhW7U5l\nbXw6j1zbpfqufkC4H6E+rnyxI7l6vxPZRVz1fz8yd9lOMgtKq0oDCYyKDKpVFebm5EBkiCc7j5+q\ndT4VlZrruocQ3daXW/u3Yc7IjnyxM5k1e878jtPzSnjp2/0Mnv8jv/9gO66OZt67qz9eLucvNTQV\nW4++mQ68r7UOA64H/qeUOicmrfUCrXU/rXW/wMDAJg9SnEfWEfhyNvynG6x/wZgR89b/wV1fQ2Dn\nSz7cTwdOciyrsPrxyl0p5JdamDmsPcM6BbAuPp3zTYmy43gOb/105JztJeUVfLkzmTHdgmvV6dY0\nOiqY3U+PZdHd/bmuewgh3i4s/f0gro0K5tmv47j2Pxt4+JNYftx/krmjOvHizT0wX0bvjiAvl6o7\n/JTq8zmSUcCe5FweuKYjr0+PZndSLn/7ci/RbX342/go3JwcmDcukj3JuXy+I6n6WK/9eIhKravv\nWm/qHUpKbgnbEo0J3N7deJS18emsjU9nzH9+ZtnW4+d8hlsTsgAY2imQt3/Xl9FdgzCbFB/MGEDr\ns+662/i58c/JvdidlMuzq+I4llVIVkEp5RXnTom9bNsJ/vndAd7dkFBr+y+HM+kX7svQjgEs3Xoc\nSx2vPVtlpebtn48w5e3NVFbCdd1DeGP9Yca/trFWlUxNWQWlPL1yH73a+DBjaET1dpNJMbF3azYe\nyiAjv5TKSs0jn8aSW1zO6j2pjH75Zx7+ZBe5xeU8NPrcv+Potj7sOnGKiqpku+lQJm5OZvq09a3e\nZ841HekR6s3jX+7hRHYRr607xMh//cTbPx+hR6g37/yuLxv+MpLwgMbtGddQ1qwaSgba1HgcVrWt\npnuB6wC01puVUi5AANCwWz9hfeUlkLQVdi2F3Z8YM0cO/AP0n3lZE7At3HiU57+Jx9fNkSUzBtAj\n1JsPNh8jMsSTfu18ScgM5rt96cSl5tGttfc5r1/8SyKrYlPoHurFsE5nbhZWxaaQW1zOtP5tznnN\nhbg6mfnv7X35YkcSZpOie6g37QPcG603x429WvO3L/eyPy2frq28WLErBaWM7cFeLoT5urL4l0Qe\nuz4SJwfjPSf0as37vyby0ncH2HE8h02HMzmRXczvBrWrrqYZExWMq6OZr3al4O/hxMs/HGRst2Ae\nG9eVeV/sZt4Xe9ibksvzN/WojmVLQjZhvq7VVS3v3tmPUkvledtjrusewj1Dwln8SyIfbTGq7lwd\nzbx1R5/qqrv8knL+88NBAL7Zk8IT47tiMilyCsuIS83j4dGd6RzswawPd7D+QMY51SFaa57/Jp4V\nu1IotVRQaqmkzFLJuO4hzJ/UE283Ryb3PcnfvtzLre9s5vHruzLjrO7AT63cR0GJhX9O7nlO4r45\nOpT//nSElbFGMt6SkM1Lt/SkTzsf5n2+h7XxJxndNYgeYef+rUW38eXD345z+GQBXUI82XQ4k4ER\nftW/JwBHs4n/TO3F9a9tYsS/fqKiUjOuewjzxkXSzv/KuPjXZM1EsA3opJSKwEgA04DbztrnODAK\neF8p1RVwAaTu50pjKTMu+nuWw/EtxgRuDi4wcBYMmQuel1en+e6Go7ywOp7RXYPYn5bPbe9u4cFR\nHYlLzeP5m7qjlOKayCCUgnXxJ+tMBPtScgGYv2Y/QzoEYDIpcovK+ce3++kZ5s2QDgGXHJfZpJjS\n79ISSH2N696KJ1fsY1VsCpEhnqzclczgDv7VvYKi2/oSXeMOE4w72adujGLy25v5encqg9r7M3No\ne6bWSHLuzg5c2y2Y1XtSiUvJxd3JzPM39SDQ05mPZw7ir5/vZvm2JB65tgs+bk5ordmakM3wLmeS\np1Lqoo3yT4yPYnjnQDILyigoKWfp1hPMXbaLVXOG0tbfjf/+dISswjLuu7o9CzYcZVtiNgPb+7P5\naBZaw5CO/vQM8yHI05mPthw7JxF8uTOZ9zYlcE1kEG393HB2MBHV2osJvVpXX+xHdAni24eG8efl\nsTz3dRz7knN5/ububE3IZtnWE3y7L41Hru1M52DPc+LvFOxJ91Av3v81gfS8UkZ3DWJKvzCUUiz/\nw1WsjU8/5/M/rU87Y/vO4zm4O5tJyCzkjjp6anUM8uT5m7rz5Y5k5o7uxKD2/hf8TG3JaolAa21R\nSs0BvsPoGrpIa71PKfUssF1rvRL4M/CuUuphjIbju3Vzmw61JSstgJj3YfObxspVAV1gwO8hfJix\n2Mll9vvXWld3xxvfoxWvTOtNVkEZd7y3hRdX78fdycxNVfX6AR7ORLfxYW18Og+O6lTrOIWlFhIy\nC4kM8WRfSh4rY1O4KTqUf3y3n+zCMt6/Z4DNBuqcj5+7E4M7+PP17lSu7RZCYlYRfxzR8aKvi27r\ny5bHR+Hj6nje0slNvUNZsSuF2KRcXp8eXd1F02RS3D0knE9jklixK4W7BodzJKOArMIyBl5iTxSz\nSdVquB8ZGcSNr29i1ocxvDY9mvc2JXBzdCgPje7E/zYfY9XuFAa29+fXI5m4O5npGeaDo9nEtAFt\nef3HQ5zILqou1RzLKuTJFfsYEO7Hu3f2u2A1nKeLI2/f0Zc31h/m5R8O8vWeVMoslfi5OzFreAf+\nMPz8pdSbo8N47us4/Nyd+L9JPasTjMmkuPYCvaLC/d3wcXOs1U4wrFPdNxq39mvDrVa6mWhMVh1H\nUDUmYPVZ256s8XMcMMSaMYgGOvozrJhjrAgVPgwmvmGM9q1R9C4pr+BEdhEa6rzrupC03BIe+2I3\n6w9kML5nK16d2hsHs4kQbxc+uW8Qc5ftYlB7Pzxq9O8f1TWYf353gPS8klr96eNT89Aa/jSmM6+u\nO8S/vj9AsJcLH285zr1DI+geem4J4kpwY8/W/OXz3by4Oh4ns4mx3evXJTPA48J974d2CqCVtwt9\n2vlyQ8/ayyF2a+1NVCsvPo05wV2Dw9mSYLQlDIy4vLvVdv7uvDotmhlLtnHzm7+ggEfHdsHNyYFr\nugaxZk8aT9/YjV8PZzEgwg/HqiQ2rX8b3vjxEH/+NJa5ozoxIMKPuct2oRT8Z1rverXFmEyKB0d1\nonuoF9/sTmNU1yBGdw2uVVVTl4m9W7N063HmXRd5SeMZlFJEt/Fhx/EcCsosBHs50yno3A4CzYmM\nLBZsOpSJUjCkY4CxfN8PT8L2Rcb8PnevhvDaufqXw5k8+mksKTV6oTx/U/c6i8enaa05VVROUk4x\nO47n8K/vD1BeUclTN0Zx11Xhte7Y/T2c+XDmwHOOMboqEayLP8ltA9tWb9+bbFQL9QzzYd64SH73\n3lbueX8rrbxd+NOYS2+0bipju4Xwt6/2sDUhm7Hdgi/Y5/xSOJpNfPfw1bg5muscvHZrvzCeXhVH\nXEoeWxOyCfJ0pp1/w7qC1jQyMoi5ozrxytpDzBnZsbqh+caerflmdypf7EzmaGZhrd9dax9X/n5D\nFK+tO8TtC7fg5+5EdmEZb9wWXa/uoTVdExl8SfM6BXg4s/ZPwy/pPU6LbuvL+gMZpOWVMCYquNnP\nPySJwM4VlVmYs3QHfuZS1g47iOm3N43lB6+aAyP/ZqzkdJaPtx6nqLyCh0d3JjzAja92JvPkir0E\neTpzbbcQtNas2p3Ka+sOkVtcTpmlkpJyo8HvtAERfrx0S89L6jXROdiDNn6urI1Pr3Ux2ZeSh7+7\nE8FezoR4uzCsUwAbD2XyzIRulzxiuCl5uzlydadA1u0/yU2NPBfRhbokTuwdyour97N8+wm2HDXq\n7hvrQvbgNZ3o186Pge3PVDWN6BKIh7MD89fsB2DwWe019wyJYPqAtqzek8rSrce5OTqUG3q2bpR4\nrCW6rdGlNL/Ect5qoebkyv0vEU3is60JTCv9nFkOqzD9WGgssD7iMQire8SnpaKSDQczuK5bCHOr\nuiyOiQpm+rtbeGDpTl6bHs0XO5L4bl863UO96B8ehJPZhLOjubo3TBtfNyJDPC+53l4pxeiuwXy8\n5ThFZWdGhe5NyaNbqHf1xezft/Zie2LOBet5rxQzhkZQVFbByEjrzUN0Nl93J0ZHBfHJthMUl1c0\n6khVk0lV99c/zcXRzLVRwXyxMxk/dyciQ86tRnRxNDOpTxiT+oQ1WizW1KuND0pR1fAtiUA0Y5YT\n2xm87l46OibyY2UfDkfez33TJ1/wNTHHcsgvsXBNjQuXm5MDi+7qxy1v/cof/heDk4OJx8ZFcu/Q\niEafPGtM12AW/5LIpkOZXNsthFJLBYfS8xlRo9dLkKcL1/dodYGjXDmGdAywyYVkSr82rN6TBsAg\nK0xZcLYberXii53JXNXe/4pruG8ILxdHOgd5opTx99bcSSKwIzmFZbg6mXGhDH58HvPm/+Khvdkx\n5E2Wp3dlx9Ec7q3U1Q10/7cmnl8OZ7Li/qHV2348cBJH87l3ff4eznwwYyDvbjzKXYPb1Zo6oTH1\nj/DD08WBtfHpXNsthEPpBVgqNd3r6FIqzu/qToEEezlTXqHp2AQNnUM7BjKsUwBT+jWPO/76+Pet\nvTA187aB0yQR2ImS8grGvrKBaHMCr7m8jfOpw3zrcj1vmG5n5ejxjN+Tyrf70tiWmM2g9v4cyShg\n4cYEKio13+5NY3xV75P1+0/SP9wPzzrqoNv6u13WDKH14Wg2MbJLEOviT1JRqasbiru1tt4U1i2R\n2aR44aYeFJRamqSh08nBxP/uPbcDQHN2pfZGawhbTzEhmshXO5KYUvQJb5b8hZycbD7s9CqzT93B\n9KuN6RJGdQ3CxdFUPU3vP9bsx8XBRBs/V97ZcAStNUk5RRxML6hVLWQLo6OCySosY9eJU+xLycPT\n2YG2DZwAzZ6NjgquHqch7JuUCOyArqzAZd1jPOr4DSWRN/NYzu2s31OGv7sTk/saRXU3JwdGRQaz\nZm8q1/doxfdx6fx5TGf8PZx5/Ms9/HY0m8Mn8wGatGGzLsM7B+JgUqyNT2dvSi5dW9tuZSchWgJJ\nBC1dZQVpH83iprJvONj+LjpPfZV3KzULNyXQIdCj1lQC43u24ps9qTywdAfBXs7MHNYepeDlHw7w\nzoYjKKCdvxvtbTxRlrerIwPb+/HdvjRST5UwfUDbi79ICHFeUjXUkhWfgs9n0urIchaqybSb/jIo\nhYPZxKzhHc6Z32VklyDcnMxkFpTx52u7GA3LjmbuHhzOTwcy2HQ4k5Fdgq6IwTOjuwZzNKOQ4vIK\naR8Q4jJJImgBKis1X+5MIiGzagpnrWHvF/DmAHTcV8wvn0bhkHk4O164AOhaNbdP7zY+3FKjP/cd\ng9rh5mSmvELbvH3gtNFdzySxltRoJ4QtSNVQC/DB5kSeXhWHScFdXRV/tryLx/EfKQvswRsBz7Ho\nsBe/Dqpf9cmLN/c4Z6UmHzcnfndVOz7dnlRrxKgttfEzBqUlZBbSIfDKm9ZXiOZEEkEzdzSjgPnf\n7mdER2/uZhWDjryHBTPPWn7HkhPXUoGZ2waGXXSisprqqvr5y9hI7h/ZEWeHhq0ZbA1zR3XiSEZB\now9aE8LeSCJoxiwVlfz501i6mZNYWLQAh+yDlHW5kZWB9xPsFMR8dycCPJ0Z2ggjV80mdUUsqVfT\nuGYyeliIK50kgmbsnQ1HaZv0Nf92XYRDmQ/c9ilOna89Z/UfIYS4EEkEV7jyikp+iEvnYHo+RzMK\nOZ5dRF5JOcXFpcwufZdXnX5Ahw2Gye9f9kphQgj7JIngCrdyVwp//jQWpSDUx5Vwf3fa+jjy+5P/\noYflJ0r7zcZ53HNgvrKqbYQQzYckgiuE1pryCn3OqkpbE7LxcXPkt8dGGYO/LGXw+QzI/QnGvojz\nVffbJmAhRIsh3S2uEIt+SWTw/HWUlFfU2r79WDZ92/qeSQKf3QPxq+C6+SBJQAjRCCQRXAG01ny8\n5RiZBWXVa8iCMW30kYxC+ob7GoPEvn4Y9n8N4/4Jg2bbMGIhREsiieAKEJeax5EMY1TwxoMZ1dtj\njuUA0K+dH2z8N+z6EIb/FQbeZ5M4hRAtk7QRXAFWxqbgYFJEtvJkw6EziWD7sRwczYreeevgx+eg\nxxRjGUkhhGhEUiKwscpKzdexqQzrFMCNPVtzML2A1NxiAGKOZTM18DhOK++HNoNgwhtwBUz4JoRo\nWSQR2FjM8RySTxUzoXdrru5srLu78WAmpZYK3JI38VTek+AbDtM+BsfmvzaqEOLKI1VDNrZyVwrO\nDibGRIXg7mQmyNOZnw9lEF22nXdML1HiGYHj3d+Au7+tQxVCtFBSIrCh8opKVu9JZXTXYDycHVBK\nMaxTIOaD39J+7e85oltTeu9awm0AACAASURBVNsK8Ai0dahCiBZMSgQ29MvhTLIKy5jQu3X1tulu\nW+ml/8VBFcE8j6dYFdz6AkcQQojLJyUCG9Fa8+7Go3i7OjKiS9Udf8wS+m5/lJ26E1OK59E5vJ1t\ngxRC2AVJBDby5c5kfjmcxSPXdjbm+N+9HFY9iOpwDS8FvkABbvQL97V1mEIIOyCJwAZyCst4/pt4\nerfx4faB7aAwC9b8xegiOn0pgzq3AaBfO0kEQgjrkzYCG3hxdTx5xeX836QemEwK1j4Jpflw4yvg\n4MzMYRF0CHKnY5CHrUMVQtgBKRE0sc1Hsvg0JomZw9rTtZUXHP8Ndn5oTCAX1BUw1gi+OTqsziUj\nhRCisUkiaEKWikqeWrmXNn6uzB3VCSrKjYnkvNsYcwgJIYQNWDURKKWuU0odUEodVkrNq+P5/yil\ndlV9HVRKnbJmPLa2fHsSB9MLeGxcV1wdTbD+RTgZB+P+AU7utg5PCGGnrNZGoJQyA28CY4AkYJtS\naqXWOu70Plrrh2vs/wAQba14bK2g1MLLPxygXztfxnULhrVPwy+vQPTvIHK8rcMTQtgxa5YIBgCH\ntdZHtdZlwDJg4gX2nw4stWI8NvX2T0fILCjjifGRqG/nGUmg3wy48TVbhyaEsHPWTAShwIkaj5Oq\ntp1DKdUOiAB+tGI8NpNyqph3Nx5lQq/W9D70Bmx9B66aA+NfBpM00wghbOtKuQpNAz7TWlfU9aRS\n6j6l1Hal1PaMjIy6drmi/ev7A2jgsatc4JfXoOc0uPZ5mVJaCHFFsGYiSAba1HgcVrWtLtO4QLWQ\n1nqB1rqf1rpfYGDzmoAtMbOQr3Ymc+egdrTa/k8wO8LopyUJCCGuGNZMBNuATkqpCKWUE8bFfuXZ\nOymlIgFfYLMVY7GZ//50GAeziT92zoW9nxtVQl6tbB2WEEJUs1oi0FpbgDnAd0A8sFxrvU8p9axS\nakKNXacBy7TW2lqx2EpSThFf7Ehmer8w/H55HtwDYciDtg5LCCFqseoUE1rr1cDqs7Y9edbjp60Z\ngy29/fMRlIIH2x6FXZtg/L/B2dPWYQkhRC0y15CVpOWWsHxbEtOjA/HfdB/4d4Q+d9k6LCGEOIck\nAivQWvPqukNUaM0jTp9D9lG4a5XRUCyEEFeYK6X7aItRZqlk3ud7WLr1OPN6FuK18x3oezdEXG3r\n0IQQok5SImhEOYVlzPowhi0J2Tw8sh0zD88EjxAY86ytQxNCiPOSRNBI0vNKmP7ubyRlF/PK1N7c\ndOoDyIiH25aDi7etwxNCiPOSRNAI0vNKmL7gN9LzSvhw5kAGeGTCqn9D98nQeaytwxNCiAuSRHCZ\n0vNKmLbgN07mlbBkxgBjeckld4KjG1z3f7YOTwghLkoSwWXQWnPXoq2czCvhg3sH0Ledn7EIfeJG\nY8yAR5CtQxRCiIuSXkOXYX9aPvvT8nns+q5GEijOge8eh9Z9oO89tg5PCCHqRUoEl+HH/ScBGBMV\nXLXheSjKgts/A5PZhpEJIUT9SYngMqzff5LuoV4Ee7lA5mHYvthYbKZ1b1uHJoQQ9SaJoIFyCsvY\ncTyHa7pUtQP89CI4OMsi9EKIZkcSQQNtOJRBpYaRkUGQtseYYnrgLGkgFkI0O5IIGujH/Sfxd3ei\nV5gP/PgCOHvLFNNCiGZJEkE9Hc0ooNRirKRpqajk54MZDO8SiCl5OxxcA0MeAFdfG0cphBCXThJB\nPSRkFjL65Z+5Y+EWcovK2XniFKeKyrkmMgjWPw9uATBwtq3DFEKIBpHuo/WwYlcyGth14hRTF2ym\nR6g3DibFcJ9MOPoTjH4GnD1sHaYQQjTIRUsESqkHlFJ2W+ehtWZlbAoDI/xYfPcATmQX8WlMEv3C\nffHc+z8wO0P072wdphBCNFh9qoaCgW1KqeVKqeuUUsraQV1J9qXkcTSjkAm9QhnaKYCl9w2irZ8b\nt0X7Qewy6HYTuPvbOkwhhGiwiyYCrfUTQCfgPeBu4JBS6kWlVAcrx3ZFWBmbgqNZMa57CAA9w3zY\n8JeRTDD9BqV5xgAyIYRoxurVWKy11kBa1ZcF8AU+U0q9ZMXYbK6yUrMqNoWrOwXi6+5U+8ntiyAo\nCtoMtE1wQgjRSOrTRjBXKRUDvAT8AvTQWs8G+gK3WDk+m9qWmE1qbgkTereu/UTyDkjdZZQG7Kum\nTAjRAtWn15AfMElrfazmRq11pVLqBuuEdWVYGZuCq6OZ0V2Daz+x/T1wdIeeU20TmBBCNKL6VA2t\nAbJPP1BKeSmlBgJoreOtFZitlVdUsnpPKqOjgnF3rpEvSwtgz+fQ4xZw8bJdgEII0UjqkwjeAgpq\nPC6o2taixZ44RU5ROeN7hNR+4tB3YCmGntNsE5gQQjSy+iQCVdVYDBhVQtjBQLSYYzkA9Av3q/1E\n3ApwD4K2g2wQlRBCNL76JIKjSqkHlVKOVV9zgaPWDszWYo7lEO7vRoCH85mNZYVw6AfoeqMsPCOE\naDHqkwhmAYOBZCAJGAjcZ82gbE1rzY7jp+jT9qwB1YfXQnmRMYhMCCFaiItW8WitTwJ2VSGelFNM\nZkEpfdqdlQjiVhgTzLUdbJvAhBDCCi6aCJRSLsC9QDfA5fR2rXWLHVJ7un2gVomgvBgOfgc9JoO5\nxTeRCCHsSH2qhv4HhABjgZ+BMCDfmkHZ2o7jObg7mekS4nlm45EfoawAoibaLjAhhLCC+iSCjlrr\nvwOFWuslwHiMdoIWK+ZYDr3b+mA21Rg1HLfCWHgmfJjtAhNCCCuoTyIor/p+SinVHfAGWuzCvIWl\nFvan5deuFrKUwoE1EHkDmB1tF5wQQlhBfSq7F1StR/AEsBLwAP5u1ahsKDbpFBWVunZD8aHvjZlG\npbeQEKIFumCJQCllAvK01jla6w1a6/Za6yCt9Tv1OXjV+gUHlFKHlVLzzrPPrUqpOKXUPqXUxw04\nh0a18/gpAPq0qZEIdi8H90CIGGGboIQQwooumAiqRhH/pSEHVkqZgTeBcUAUMF0pFXXWPp2Ax4Ah\nWutuwEMNea/GFHMsh45BHni7VVUBleQavYW63yK9hYQQLVJ92gjWKqUeUUq1UUr5nf6qx+sGAIe1\n1ke11mXAMuDsLje/B97UWudA9ZgFm9Fas/N4Dn3a+pzZGL8KKkqhxxTbBSaEEFZUn1vc03Mt319j\nmwbaX+R1ocCJGo9Pj0quqTOAUuoXwAw8rbX+9uwDKaXuo2o0c9u2besRcsMkZBaSU1RO33ZnVQv5\nRkBoX6u9rxBC2FJ9RhZHWPn9OwEjMMYnbFBK9dBanzorhgXAAoB+/frpsw/SWPam5AHQI7SqRJCX\nCgkbYPhfZAEaIUSLVZ+RxXfWtV1r/cFFXpoMtKnxOKxqW01JwBatdTmQoJQ6iJEYtl0sLmvYn5qH\ng0nRMcjD2LDvC0BLtZAQokWrT9VQ/xo/uwCjgB3AxRLBNqCTUioCIwFMA247a5+vgOnAYqVUAEZV\nkc1mNt2flk/HIA+cHKqaTnYvh1a9IaCTrUISQgirq0/V0AM1HyulfDAafi/2OotSag7wHUb9/yKt\n9T6l1LPAdq31yqrnrlVKxQEVwKNa66wGnEejOJCWT//wqvaB7KPGusTXvmCrcIQQokk0pD9kIVCv\ndgOt9Wpg9Vnbnqzxswb+VPVlU7nF5SSfKuZ3rdoZG47+ZHzvfJ3NYhJCiKZQnzaCVRi9hMDobhoF\nLLdmULZwIM2YR696ormEDeDZGvw72DAqIYSwvvqUCP5V42cLcExrnWSleGxmf5rRY6hriBdUVhqJ\noOMY6S0khGjx6pMIjgOpWusSAKWUq1IqXGudaNXImlh8aj4+bo4EezlD+j4oyoL2w20dlhBCWF19\nRhZ/ClTWeFxRta1F2Z+WR2SIJ0opozQAMuW0EMIu1CcROFRNEQFA1c9O1gup6VVWag6k5RMZ4mVs\nSNgAfu3Bp82FXyiEEC1AfRJBhlJqwukHSqmJQKb1Qmp6J3KKKCqroGsrT6iwwLFfIOJqW4clhBBN\noj5tBLOAj5RSb1Q9TgLqHG3cXMWnGj2GIkO8IDXWWHtAEoEQwk7UZ0DZEWCQUsqj6nGB1aNqYgfS\n8lEKOgd7wpafjY3SPiCEsBMXrRpSSr2olPLRWhdorQuUUr5KqeebIrimsj8tj3B/d1ydzEb7QFAU\neLTY1TiFEKKW+rQRjKs5G2jV2gHXWy+kprc/LZ/IEE9jbeLjv0m1kBDCrtQnEZiVUs6nHyilXAHn\nC+zfrBSVWUjMKjTaB1J2gqVYqoWEEHalPo3FHwHrlFKLAQXcDSyxZlBN6WB6AVpDZCtPSPvJ2Ng6\n2qYxCSFEU6pPY/E/lFKxwGiMOYe+A9pZO7Cm8vOBDJSC3m184Oe94OIDXq1tHZYQQjSZ+lQNAaRj\nJIEpwDVAvNUiakJaa77alcygCH+CvVyMqSWCu8v8QkIIu3LeEoFSqjPGojHTMQaQfQIorfXIJorN\n6nYn5ZKQWcis4e2NiebS4yD6DluHJYQQTepCVUP7gY3ADVrrwwBKqYebJKom8uXOZJwcTFzXvRWc\nSoTyQgjpbuuwhBCiSV2oamgSkAqsV0q9q5QahdFY3CJYKir5encKoyKD8HZ1NKqFAIK72TYwIYRo\nYudNBFrrr7TW04BIYD3wEBCklHpLKXVtUwVoLZsOZ5JZUMZN0aHGhvR9gILArjaNSwghmtpFG4u1\n1oVa64+11jcCYcBO4K9Wj8zKvtqZjJeLAyO6BBob0vYYq5E5udk2MCGEaGL17TUEGKOKtdYLtNaj\nrBVQUygstfDdvnTG92yNs4PZ2Ji+T6qFhBB26ZISQUvx88EMissrmNi7arxAaQHkJBhdR4UQws7Y\nZSJIyCwEoGeYt7HhZNWwCCkRCCHskF0mgpN5JXi6OODmVNV7Nn2v8V1KBEIIO2SXiSAtr4QQL5cz\nG9L3gZMn+LS1XVBCCGEjdpkI0vNKjSklqjdUNRTL1BJCCDtkp4mg5Ewi0Fp6DAkh7JrdJYKKSs3J\n/FJCvKuWVMhNgtJcSQRCCLtld4kgq7CUikp9pkRwMs74LolACGGn7C4RpOeWApxJBFmHje8BnW0U\nkRBC2Jb9JYK8EqBmIjhiLEbj5mfDqIQQwnbsLhGkVSWC6u6j2UfBr70NIxJCCNuyu0RwMq8Ek4IA\nDydjQ/YRY7I5IYSwU3aXCNLySgjwcMbBbAJLqdFrSEoEQgg7ZoeJoJQQ76pqoZxjoCvBT0oEQgj7\nZdVEoJS6Til1QCl1WCk1r47n71ZKZSildlV9zbRmPGBUDQV51mgfACkRCCHs2oXWLL4sSikz8CYw\nBkgCtimlVmqt487a9ROt9RxrxXG2tLwS+oX7Gg+yjxjfpY1ACGHHrFkiGAAc1lof1VqXAcuAiVZ8\nv4sqKa/gVFE5wZ41u456g6uvLcMSQgibsmYiCAVO1HicVLXtbLcopXYrpT5TSrWp60BKqfuUUtuV\nUtszMjIaHNDJvKrBZN41u452kMnmhBB2zdaNxauAcK11T+AHYEldO1Utj9lPa90vMDCwwW+Wnn/2\nGIIj0j4ghLB71kwEyUDNO/ywqm3VtNZZWuvSqocLgb5WjIe03Bqjik93HZX2ASGEnbNmItgGdFJK\nRSilnIBpwMqaOyilWtV4OAGIt2I81dNLhHi51Og6KiUCIYR9s1qvIa21RSk1B/gOMAOLtNb7lFLP\nAtu11iuBB5VSEwALkA3cba14wEgEzg4mvFwd4MTprqNSIhBC2DerJQIArfVqYPVZ256s8fNjwGPW\njKGm04PJlFJnuo5KiUAIYeds3VjcpGqtTJZ91Og6KrOOCiHsnP0mgqyqHkPSdVQIYefsJhForUnL\nLSHEq2qJytNjCIQQws7ZTSLIK7ZQaqms6jpaBrknpOuoEEJgR4kgrebKZKek66gQQpxmN4mg1hKV\nWad7DEmJQAgh7CYR1FqiMi/J2OhT59RGQghhV+wmEZysSgRBXs5QnGNslFlHhRDCugPKriR/GN6B\nW/qG4eJohuJT4OgODs62DksIIWzObkoEjmYTrbxdjQfFOVIaEEKIKnaTCGqRRCCEENXsOBH42DoK\nIYS4IthxIpASgRBCgCQCIYSwe/aXCLSWRCCEEDXYXyIoL4KKMkkEQghRxf4SgQwmE0KIWiQRCCGE\nnZNEIIQQdk4SgRBC2DlJBEIIYeckEQghhJ2zm9lHqxXngNkZHF1tHYkQl6W8vJykpCRKSkpsHYq4\ngri4uBAWFoajo2O9X2OficDVF5SydSRCXJakpCQ8PT0JDw9Hyd+zALTWZGVlkZSURERERL1fZ59V\nQ1ItJFqAkpIS/P39JQmIakop/P39L7mUaIeJ4JQkAtFiSBIQZ2vI34QdJgIpEQghRE2SCIQQDTJy\n5Ei+++67WtteeeUVZs+efcHXeXh4AJCSksLkyZPr3GfEiBFs3779gsd55ZVXKCoqqn58/fXXc+rU\nqfqELs5ip4lAFqUR4nJNnz6dZcuW1dq2bNkypk+fXq/Xt27dms8++6zB7392Ili9ejU+Ps3nf1tr\nTWVlpa3DAOyt11B5iTH7qJQIRAvzzKp9xKXkNeoxo1p78dSN3c77/OTJk3niiScoKyvDycmJxMRE\nUlJSGDZsGAUFBUycOJGcnBzKy8t5/vnnmThxYq3XJyYmcsMNN7B3716Ki4u55557iI2NJTIykuLi\n4ur9Zs+ezbZt2yguLmby5Mk888wzvPbaa6SkpDBy5EgCAgJYv3494eHhbN++nYCAAF5++WUWLVoE\nwMyZM3nooYdITExk3LhxDB06lF9//ZXQ0FBWrFiBq2vtruSrVq3i+eefp6ysDH9/fz766COCg4Mp\nKCjggQceYPv27SileOqpp7jlllv49ttvefzxx6moqCAgIIB169bx9NNP4+HhwSOPPAJA9+7d+frr\nrwEYO3YsAwcOJCYmhtWrVzN//vxzzg9g27ZtzJ07l8LCQpydnVm3bh3jx4/ntddeo3fv3gAMHTqU\nN998k169el3W79q+EkFJVbFREoEQl83Pz48BAwawZs0aJk6cyLJly7j11ltRSuHi4sKXX36Jl5cX\nmZmZDBo0iAkTJpy3IfOtt97Czc2N+Ph4du/eTZ8+faqfe+GFF/Dz86OiooJRo0axe/duHnzwQV5+\n+WXWr19PQEBArWPFxMSwePFitmzZgtaagQMHMnz4cHx9fTl06BBLly7l3Xff5dZbb+Xzzz/njjvu\nqPX6oUOH8ttvv6GUYuHChbz00kv8+9//5rnnnsPb25s9e/YAkJOTQ0ZGBr///e/ZsGEDERERZGdn\nX/RzO3ToEEuWLGHQoEHnPb/IyEimTp3KJ598Qv/+/cnLy8PV1ZV7772X999/n1deeYWDBw9SUlJy\n2UkA7C0RyKhi0UJd6M7dmk5XD51OBO+99x5gVHs8/vjjbNiwAZPJRHJyMunp6YSEhNR5nA0bNvDg\ngw8C0LNnT3r27Fn93PLly1mwYAEWi4XU1FTi4uJqPX+2TZs2cfPNN+Pu7g7ApEmT2LhxIxMmTCAi\nIqL6brpv374kJiae8/qkpCSmTp1KamoqZWVl1f3x165dW6sqzNfXl1WrVnH11VdX7+Pn53fRz6xd\nu3bVSeB856eUolWrVvTv3x8ALy8vAKZMmcJzzz3HP//5TxYtWsTdd9990ferD/tqI5BEIESjmjhx\nIuvWrWPHjh0UFRXRt29fAD766CMyMjKIiYlh165dBAcHN2gEdEJCAv/6179Yt24du3fvZvz48Zc1\nktrZ2bn6Z7PZjMViOWefBx54gDlz5rBnzx7eeeedBr2fg4NDrfr/msc4naDg0s/Pzc2NMWPGsGLF\nCpYvX87tt99+ybHVxaqJQCl1nVLqgFLqsFJq3gX2u0UppZVS/awZjyQCIRqXh4cHI0eOZMaMGbUa\niXNzcwkKCsLR0ZH169dz7NixCx7n6quv5uOPPwZg79697N69G4C8vDzc3d3x9vYmPT2dNWvWVL/G\n09OT/Pz8c441bNgwvvrqK4qKiigsLOTLL79k2LBh9T6n3NxcQkNDAViyZEn19jFjxvDmm29WP87J\nyWHQoEFs2LCBhIQEgOqqofDwcHbs2AHAjh07qp8/2/nOr0uXLqSmprJt2zYA8vPzq5PWzJkzefDB\nB+nfvz++vo1zLbNaIlBKmYE3gXFAFDBdKRVVx36ewFxgi7ViqSaJQIhGN336dGJjY2slgttvv53t\n27fTo0cPPvjgAyIjIy94jNmzZ1NQUEDXrl158sknq0sWvXr1Ijo6msjISG677TaGDBlS/Zr77ruP\n6667jpEjR9Y6Vp8+fbj77rsZMGAAAwcOZObMmURHR9f7fJ5++mmmTJlC3759a7U/PPHEE+Tk5NC9\ne3d69erF+vXrCQwMZMGCBUyaNIlevXoxdepUAG655Rays7Pp1q0bb7zxBp07d67zvc53fk5OTnzy\nySc88MAD9OrVizFjxlSXFPr27YuXlxf33HNPvc/pYpTWutEOVuvASl0FPK21Hlv1+DEArfX/nbXf\nK8APwKPAI1rrC3Ye7tevn75Y/+Lz+vV1+P4JmHcCXLwadgwhrhDx8fF07drV1mGIJpaSksKIESPY\nv38/JlPd9/J1/W0opWK01nXWulizaigUOFHjcVLVtpqB9QHaaK2/udCBlFL3KaW2K6W2Z2RkNDyi\n4hxQZnD2bPgxhBDCRj744AMGDhzICy+8cN4k0BA2ayxWSpmAl4E/X2xfrfUCrXU/rXW/wMDAhr+p\nzDwqhGjG7rzzTk6cOMGUKVMa9bjWTATJQJsaj8Oqtp3mCXQHflJKJQKDgJVWbTCW6SWEEOIc1kwE\n24BOSqkIpZQTMA1YefpJrXWu1jpAax2utQ4HfgMmXKyN4LJIIhBCiHNYLRForS3AHOA7IB5YrrXe\np5R6Vik1wVrve0GSCIQQ4hxWHVmstV4NrD5r25Pn2XeENWMBjEQQKL0shBCiJjsbWSyL0gjRWLKy\nsujduze9e/cmJCSE0NDQ6sdlZWX1OsY999zDgQMHLrjPm2++yUcffdQYIQOQnp6Og4MDCxcubLRj\nNnf2M9dQRTmU5kkiEKKR+Pv7s2vXLoBzZts8TWuN1vq8XR0XL1580fe5//77Lz/YGpYvX85VV13F\n0qVLmTlzZqMeuyaLxYKDQ/O4xDaPKBtDSa7xXRKBaInWzIO0PY17zJAeMG7+Jb/s8OHDTJgwgejo\naHbu3MkPP/zAM888w44dOyguLmbq1Kk8+aRRQzx06FDeeOMNunfvTkBAALNmzWLNmjW4ubmxYsUK\ngoKCeOKJJwgICOChhx5i6NChDB06lB9//JHc3FwWL17M4MGDKSws5M477yQ+Pp6oqCgSExNZuHBh\n9QRzNS1dupTXX3+dyZMnk5qaSqtWrQD45ptv+Pvf/05FRQXBwcF8//335OfnM2fOHHbu3AnAs88+\nyw033EBAQED1IjjLli1j7dq1LFy4kDvuuANPT09iYmIYMWIEkyZN4uGHH6akpAQ3Nzfef/99OnXq\nhMVi4dFHH+WHH37AZDIxa9YsOnbsyIIFC6rXaFizZg2LFi3i008/bdCv71LYTyIolimohWgq+/fv\n54MPPqBfP6M3+Pz58/Hz88NisTBy5EgmT55MVFTtGWdyc3MZPnw48+fP509/+hOLFi1i3rxzpyjT\nWrN161ZWrlzJs88+y7fffsvrr79OSEgIn3/+ObGxsbWmsa4pMTGR7Oxs+vbty5QpU1i+fDlz584l\nLS2N2bNns3HjRtq1a1c9Z9DTTz9NYGAgu3fvRmtdrxXQUlNT+e233zCZTOTm5rJx40YcHBz49ttv\neeKJJ/jkk0946623SElJITY2FrPZTHZ2Nj4+PsyZM4esrCz8/f1ZvHgxM2bMuNSPvkHsKBHIPEOi\nBWvAnbs1dejQoToJgHEX/t5772GxWEhJSSEuLu6cRODq6sq4ceMAYz6djRs31nnsSZMmVe9zehrp\nTZs28de//hUw5u/p1q3uabmXLVtWPR/QtGnT+OMf/8jcuXPZvHkzI0eOpF27dsCZ6aTXrl3LV199\nBRiLwvv6+tY5Y2lNU6ZMqa4KO3XqFHfeeSdHjhyptc/atWt56KGHMJvNtd7v9ttv5+OPP+b2228n\nJiaGpUuXXvC9GoskAiFEo6s51fKhQ4d49dVX2bp1Kz4+Ptxxxx11TrXs5ORU/fP5poiGM1NJX2if\n81m6dCmZmZnVs4qmpKRw9OjRSzqGyWSi5hxtZ59LzXP/29/+xtixY/njH//I4cOHue666y547Bkz\nZnDLLbcAMHXq1OpEYW3202uoOhE0nzVNhWgJ8vLy8PT0xMvLi9TU1HMWvG8MQ4YMYfny5QDs2bOH\nuLi4c/aJi4vDYrGQnJxMYmIiiYmJPProoyxbtozBgwfXmi77dNVQzamntdbk5ORgMpmqVzurrKzk\nyy+/PG9cNae0fv/996u3jxkzhrfffpuKiopa79emTRsCAgKYP39+oy06Ux92mAikRCBEU+rTpw9R\nUVFERkZy55131ppKurE88MADJCcnExUVxTPPPENUVBTe3t619lm6dCk333xzrW233HILS5cuJTg4\nmLfeeouJEyfSq1ev6gVfnnrqKdLT0+nevTu9e/eurq76xz/+wdixYxk8eDBhYWHnjeuvf/0rjz76\nKH369KlVivjDH/5ASEgIPXv2pFevXtVJDOC2224jIiLivFNXW4PVpqG2lgZPQ73/G9j1Mdz6AZia\nprglhDXJNNRnWCwWLBYLLi4uHDp0iGuvvZZDhw41m+6bNc2aNYurrrqKu+66q8HHuNRpqJvfp9RQ\nkeONLyFEi1NQUMCoUaOwWCxorXnnnXeaZRLo3bs3vr6+vPbaa036vs3vkxJCiLP4+PgQExNj6zAu\n2+kBek3NftoIhGiBmlvVrrC+hvxNSCIQoplycXEhKytLkoGoprUmKysLFxeXS3qdVA0J0UyFhYWR\nlJTEZS3fKlocFxeXdtqabgAABdhJREFUC/ZkqoskAiGaKUdHRyIiImwdhmgBpGpICCHsnCQCIYSw\nc5IIhBDCzjW7kcVKqQzgWANfHgBkNmI4zYU9nrc9njPY53nb4znDpZ93O611YF1PNLtEcDmUUtvP\nN8S6JbPH87bHcwb7PG97PGdo3POWqiEhhLBzkgiEEMLO2VsiWGDrAGzEHs/bHs8Z7PO87fGcoRHP\n267aCIQQQpzL3koEQgghziKJQAgh7JzdJAKl1HVKqQNKqcNKqXm2jscalFJtlFLrlVJxSql9Sqm5\nVdv9lFI/KKUOVX1vcet1KqXMSqmdSqmvqx5HKKW2VP2+P1FKOV3sGM2NUspHKfWZUmq/UipeKfX/\n7d1bqBVVHMfx7w+P2VFBzUDMU5zCQ2EXL0jYhQjrIS0yKDARkhACibKIbvQU9FJEF0uE0spCDDIr\n6UGqo1RQaRdMLaPUxAvHVMrThfDWr4e1jM3x7HLj2Y7N/D8w7Flrhj1r8d/s/541s9dcVpFY35s/\n3xslLZV0etniLeklSXskbayp6zW2Sublvq+XNKHR41UiEUjqB8wHpgBjgBmSxhTbqqY4DNxnewww\nCbgz9/MhoNN2B9CZy2UzF9hUU34ceNr2aOAXYHYhrWquZ4GVti8AxpL6X+pYSxoF3A1MtH0R0A+4\nlfLF+xXguh519WI7BejIyx3AgkYPVolEAFwKbLa91fZB4HVgWsFt6nO2u2x/ldd/I30xjCL1dXHe\nbTFwUzEtbA5JbcD1wMJcFjAZWJZ3KWOfhwBXAYsAbB+0vZ+SxzprAVoltQADgS5KFm/bHwE/96iu\nF9tpwKtOPgOGShrZyPGqkghGATtqyjtzXWlJagfGA2uAEba78qbdwIiCmtUszwAPAH/l8nBgv+3D\nuVzGeJ8L7AVezkNiCyUNouSxtr0LeBLYTkoA3cCXlD/eUD+2J/z9VpVEUCmSBgNvAvfY/rV2m9P9\nwqW5Z1jSDcAe2///B9Y2pgWYACywPR74gx7DQGWLNUAeF59GSoRnAYM4dgil9Po6tlVJBLuAs2vK\nbbmudCT1JyWBJbaX5+qfjp4q5tc9RbWvCa4AbpS0jTTkN5k0dj40Dx1AOeO9E9hpe00uLyMlhjLH\nGuBa4Efbe20fApaTPgNljzfUj+0Jf79VJRF8DnTkOwtOI11cWlFwm/pcHhtfBGyy/VTNphXArLw+\nC3jnZLetWWw/bLvNdjsprqtszwRWA7fk3UrVZwDbu4Edks7PVdcA31LiWGfbgUmSBubP+9F+lzre\nWb3YrgBuy3cPTQK6a4aQjo/tSizAVOB7YAvwSNHtaVIfrySdLq4H1uVlKmnMvBP4AfgAOKPotjap\n/1cD7+b184C1wGbgDWBA0e1rQn/HAV/keL8NDKtCrIFHge+AjcBrwICyxRtYSroGcoh09je7XmwB\nke6K3AJsIN1R1dDxYoqJEEKouKoMDYUQQqgjEkEIIVRcJIIQQqi4SAQhhFBxkQhCCKHiIhGE0IOk\nI5LW1Sx9NnGbpPbaGSVDOBW0/PcuIVTOn7bHFd2IEE6WOCMI4ThJ2ibpCUkbJK2VNDrXt0taleeC\n75R0Tq4fIektSV/n5fL8Vv0kvZjn1H9PUmthnQqBSAQh9Ka1x9DQ9Jpt3bYvBp4nzXoK8Byw2PYl\nwBJgXq6fB3xoeyxpHqBvcn0HMN/2hcB+4OYm9yeEfxX/LA6hB0m/2x7cS/02YLLtrXlyv922h0va\nB4y0fSjXd9k+U9JeoM32gZr3aAfed3q4CJIeBPrbfqz5PQuhd3FGEEJjXGe9EQdq1o8Q1+pCwSIR\nhNCY6TWvn+b1T0gznwLMBD7O653AHPjnmcpDTlYjQ2hE/BIJ4VitktbVlFfaPnoL6TBJ60m/6mfk\nurtITwq7n/TUsNtz/VzgBUmzSb/855BmlAzhlBLXCEI4TvkawUTb+4puSwh9KYaGQgih4uKMIIQQ\nKi7OCEIIoeIiEYQQQsVFIgghhIqLRBBCCBUXiSCEECrub9XLbyGcyWyyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olCixXBK8YCl",
        "colab_type": "code",
        "outputId": "5432d60e-8095-414a-aa06-382494056bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image as pil_image\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras import layers\n",
        "from keras.applications import vgg16\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"utility function to normalize a tensor.\n",
        "\n",
        "    # Arguments\n",
        "        x: An input tensor.\n",
        "\n",
        "    # Returns\n",
        "        The normalized input tensor.\n",
        "    \"\"\"\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
        "\n",
        "    # Arguments\n",
        "        x: A numpy-array representing the generated image.\n",
        "\n",
        "    # Returns\n",
        "        A processed numpy-array, which could be used in e.g. imshow.\n",
        "    \"\"\"\n",
        "    # normalize tensor: center on 0., ensure std is 0.25\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + K.epsilon())\n",
        "    x *= 0.25\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "\n",
        "def process_image(x, former):\n",
        "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
        "       Reverses `deprocess_image`.\n",
        "\n",
        "    # Arguments\n",
        "        x: A numpy-array, which could be used in e.g. imshow.\n",
        "        former: The former numpy-array.\n",
        "                Need to determine the former mean and variance.\n",
        "\n",
        "    # Returns\n",
        "        A processed numpy-array representing the generated image.\n",
        "    \"\"\"\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((2, 0, 1))\n",
        "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n",
        "\n",
        "\n",
        "def visualize_layer(model,\n",
        "                    layer_name,\n",
        "                    step=1.,\n",
        "                    epochs=15,\n",
        "                    upscaling_steps=9,\n",
        "                    upscaling_factor=1.2,\n",
        "                    output_dim=(412, 412),\n",
        "                    filter_range=(0, None)):\n",
        "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
        "\n",
        "    # Arguments\n",
        "        model: The model containing layer_name.\n",
        "        layer_name: The name of the layer to be visualized.\n",
        "                    Has to be a part of model.\n",
        "        step: step size for gradient ascent.\n",
        "        epochs: Number of iterations for gradient ascent.\n",
        "        upscaling_steps: Number of upscaling steps.\n",
        "                         Starting image is in this case (80, 80).\n",
        "        upscaling_factor: Factor to which to slowly upgrade\n",
        "                          the image towards output_dim.\n",
        "        output_dim: [img_width, img_height] The output image dimensions.\n",
        "        filter_range: Tupel[lower, upper]\n",
        "                      Determines the to be computed filter numbers.\n",
        "                      If the second value is `None`,\n",
        "                      the last filter will be inferred as the upper boundary.\n",
        "    \"\"\"\n",
        "\n",
        "    def _generate_filter_image(input_img,\n",
        "                               layer_output,\n",
        "                               filter_index):\n",
        "        \"\"\"Generates image for one particular filter.\n",
        "\n",
        "        # Arguments\n",
        "            input_img: The input-image Tensor.\n",
        "            layer_output: The output-image Tensor.\n",
        "            filter_index: The to be processed filter number.\n",
        "                          Assumed to be valid.\n",
        "\n",
        "        #Returns\n",
        "            Either None if no image could be generated.\n",
        "            or a tuple of the image (array) itself and the last loss.\n",
        "        \"\"\"\n",
        "        s_time = time.time()\n",
        "\n",
        "        # we build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
        "        else:\n",
        "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # we compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, input_img)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads = normalize(grads)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([input_img], [loss, grads])\n",
        "\n",
        "        # we start from a gray image with some random noise\n",
        "        intermediate_dim = tuple(\n",
        "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            input_img_data = np.random.random(\n",
        "                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n",
        "        else:\n",
        "            input_img_data = np.random.random(\n",
        "                (1, intermediate_dim[0], intermediate_dim[1], 3))\n",
        "        input_img_data = (input_img_data - 0.5) * 20 + 128\n",
        "\n",
        "        # Slowly upscaling towards the original size prevents\n",
        "        # a dominating high-frequency of the to visualized structure\n",
        "        # as it would occur if we directly compute the 412d-image.\n",
        "        # Behaves as a better starting point for each following dimension\n",
        "        # and therefore avoids poor local minima\n",
        "        for up in reversed(range(upscaling_steps)):\n",
        "            # we run gradient ascent for e.g. 20 steps\n",
        "            for _ in range(epochs):\n",
        "                loss_value, grads_value = iterate([input_img_data])\n",
        "                input_img_data += grads_value * step\n",
        "\n",
        "                # some filters get stuck to 0, we can skip them\n",
        "                if loss_value <= K.epsilon():\n",
        "                    return None\n",
        "\n",
        "            # Calculate upscaled dimension\n",
        "            intermediate_dim = tuple(\n",
        "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
        "            # Upscale\n",
        "            img = deprocess_image(input_img_data[0])\n",
        "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
        "                                                           pil_image.BICUBIC))\n",
        "            input_img_data = np.expand_dims(\n",
        "                process_image(img, input_img_data[0]), 0)\n",
        "\n",
        "        # decode the resulting input image\n",
        "        img = deprocess_image(input_img_data[0])\n",
        "        e_time = time.time()\n",
        "        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
        "                                                                  loss_value,\n",
        "                                                                  e_time - s_time))\n",
        "        return img, loss_value\n",
        "\n",
        "    def _draw_filters(filters, n=None):\n",
        "        \"\"\"Draw the best filters in a nxn grid.\n",
        "\n",
        "        # Arguments\n",
        "            filters: A List of generated images and their corresponding losses\n",
        "                     for each processed filter.\n",
        "            n: dimension of the grid.\n",
        "               If none, the largest possible square will be used\n",
        "        \"\"\"\n",
        "        if n is None:\n",
        "            n = int(np.floor(np.sqrt(len(filters))))\n",
        "\n",
        "        # the filters that have the highest loss are assumed to be better-looking.\n",
        "        # we will only keep the top n*n filters.\n",
        "        filters.sort(key=lambda x: x[1], reverse=True)\n",
        "        filters = filters[:n * n]\n",
        "\n",
        "        # build a black picture with enough space for\n",
        "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
        "        MARGIN = 5\n",
        "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
        "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
        "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
        "\n",
        "        # fill the picture with our saved filters\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                img, _ = filters[i * n + j]\n",
        "                width_margin = (output_dim[0] + MARGIN) * i\n",
        "                height_margin = (output_dim[1] + MARGIN) * j\n",
        "                stitched_filters[\n",
        "                    width_margin: width_margin + output_dim[0],\n",
        "                    height_margin: height_margin + output_dim[1], :] = img\n",
        "\n",
        "        # save the result to disk\n",
        "        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
        "\n",
        "    # this is the placeholder for the input images\n",
        "    assert len(model.inputs) == 1\n",
        "    input_img = model.inputs[0]\n",
        "\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "    output_layer = layer_dict[layer_name]\n",
        "    assert isinstance(output_layer, layers.Conv2D)\n",
        "\n",
        "    # Compute to be processed filter range\n",
        "    filter_lower = filter_range[0]\n",
        "    filter_upper = (filter_range[1]\n",
        "                    if filter_range[1] is not None\n",
        "                    else len(output_layer.get_weights()[1]))\n",
        "    assert(filter_lower >= 0\n",
        "           and filter_upper <= len(output_layer.get_weights()[1])\n",
        "           and filter_upper > filter_lower)\n",
        "    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n",
        "\n",
        "    # iterate through each filter and generate its corresponding image\n",
        "    processed_filters = []\n",
        "    for f in range(filter_lower, filter_upper):\n",
        "        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n",
        "\n",
        "        if img_loss is not None:\n",
        "            processed_filters.append(img_loss)\n",
        "\n",
        "    print('{} filter processed.'.format(len(processed_filters)))\n",
        "    # Finally draw and store the best filters to disk\n",
        "    _draw_filters(processed_filters)\n",
        "\n",
        "\n",
        "# the name of the layer we want to visualize\n",
        "# (see model definition at keras/applications/vgg16.py)\n",
        "LAYER_NAME = 'conv2'\n",
        "\n",
        "# build the VGG16 network with ImageNet weights\n",
        "\n",
        "\n",
        "# example function call\n",
        "visualize_layer(model, LAYER_NAME)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute filters 0 to 32\n",
            "Costs of filter   4:    57 ( 0.74s )\n",
            "Costs of filter   8:    11 ( 0.42s )\n",
            "Costs of filter  14:     6 ( 0.41s )\n",
            "Costs of filter  18:     9 ( 0.43s )\n",
            "Costs of filter  24:    74 ( 0.46s )\n",
            "Costs of filter  31:     6 ( 0.44s )\n",
            "6 filter processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ZGm7ANAOSh",
        "colab_type": "code",
        "outputId": "84971be8-2878-408c-d55e-752ab10bce56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.layers[1].name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'activation_1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiXmabFsJbb8",
        "colab_type": "code",
        "outputId": "6824256d-43b7-4537-9eac-191b16b96a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "pred_test=model.predict_classes(x_test)\n",
        "conf=confusion_matrix(true_y_test,pred_test)\n",
        "print(conf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[865  10  26   9  18   3   5  14  20  30]\n",
            " [ 16 892   2   3   0   0   2   1  11  73]\n",
            " [ 58   2 671  18 100  37  66  25   6  17]\n",
            " [ 23   8  59 508 103 109 108  45  10  27]\n",
            " [ 10   2  27   9 844  13  49  42   3   1]\n",
            " [  7   6  38  74  53 695  46  62   2  17]\n",
            " [  5   1  18  13  28   7 917   3   4   4]\n",
            " [  6   0  16  16  34  23   6 885   3  11]\n",
            " [ 77  18   8   3   4   5   2   4 848  31]\n",
            " [ 28  29   3   1   1   0   2   7   9 920]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjDhtu8Y0iE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcnHVX1h0lCR",
        "colab_type": "code",
        "outputId": "1690e25b-4181-401b-8c5e-78db4cff386d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "a=[[865,10,26  ,  9,  18,   3,   5,  14,  20,  30,],\n",
        " [ 16, 892,   2,   3,   0,   0,   2,   1,  11,  73],\n",
        " [ 58,   2, 671,  18, 100,  37,  66,  25,   6,  17],\n",
        " [ 23,   8,  59, 508, 103, 109, 108,  45,  10,  27],\n",
        " [ 10,   2,  27,   9, 844,  13,  49,  42,   3,   1],\n",
        " [  7,   6,  38,  74,  53, 695,  46,  62,   2,  17],\n",
        " [  5,   1,  18,  13,  28,   7, 917,   3,  4,   4],\n",
        " [  6,   0 , 16,  16,  34,  23,   6, 885,   3,  11],\n",
        " [ 77,  18,   8,   3,   4,   5,   2,   4, 848,  31],\n",
        " [ 28,  29,   3,   1,   1,   0   ,2,   7,   9, 920]]\n",
        "a=np.array(a)\n",
        "a\n",
        "names=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "df=pd.DataFrame(a, columns=names, index=names)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airplane</th>\n",
              "      <th>automobile</th>\n",
              "      <th>bird</th>\n",
              "      <th>cat</th>\n",
              "      <th>deer</th>\n",
              "      <th>dog</th>\n",
              "      <th>frog</th>\n",
              "      <th>horse</th>\n",
              "      <th>ship</th>\n",
              "      <th>truck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>airplane</th>\n",
              "      <td>865</td>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>automobile</th>\n",
              "      <td>16</td>\n",
              "      <td>892</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bird</th>\n",
              "      <td>58</td>\n",
              "      <td>2</td>\n",
              "      <td>671</td>\n",
              "      <td>18</td>\n",
              "      <td>100</td>\n",
              "      <td>37</td>\n",
              "      <td>66</td>\n",
              "      <td>25</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>23</td>\n",
              "      <td>8</td>\n",
              "      <td>59</td>\n",
              "      <td>508</td>\n",
              "      <td>103</td>\n",
              "      <td>109</td>\n",
              "      <td>108</td>\n",
              "      <td>45</td>\n",
              "      <td>10</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deer</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "      <td>9</td>\n",
              "      <td>844</td>\n",
              "      <td>13</td>\n",
              "      <td>49</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>38</td>\n",
              "      <td>74</td>\n",
              "      <td>53</td>\n",
              "      <td>695</td>\n",
              "      <td>46</td>\n",
              "      <td>62</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>frog</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>13</td>\n",
              "      <td>28</td>\n",
              "      <td>7</td>\n",
              "      <td>917</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>horse</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>34</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>885</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ship</th>\n",
              "      <td>77</td>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>848</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>truck</th>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            airplane  automobile  bird  cat  ...  frog  horse  ship  truck\n",
              "airplane         865          10    26    9  ...     5     14    20     30\n",
              "automobile        16         892     2    3  ...     2      1    11     73\n",
              "bird              58           2   671   18  ...    66     25     6     17\n",
              "cat               23           8    59  508  ...   108     45    10     27\n",
              "deer              10           2    27    9  ...    49     42     3      1\n",
              "dog                7           6    38   74  ...    46     62     2     17\n",
              "frog               5           1    18   13  ...   917      3     4      4\n",
              "horse              6           0    16   16  ...     6    885     3     11\n",
              "ship              77          18     8    3  ...     2      4   848     31\n",
              "truck             28          29     3    1  ...     2      7     9    920\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljE10wWr4_fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fa191416-4321-446c-cdfa-c996e08129d9",
        "id": "XvW1Eko74__Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "a=[[459,  40,  47,  42,  73,  20,  64,  37, 160,  58],\n",
        " [ 60, 352,  26,  57,  41,  48,  83,  40, 101, 192],\n",
        " [ 99,  42, 122,  90, 250,  56, 215,  63,  27,  36],\n",
        " [ 54,  47,  67, 255,  88, 133, 187,  68,  36,  65],\n",
        " [ 48,  21,  63,  50, 422,  28, 238,  79,  29,  22],\n",
        " [ 40,  24,  74, 173,  87, 288, 150,  86,  31,  47],\n",
        " [ 21,  38,  50,  46, 150,  42, 565,  38,  10,  40],\n",
        " [ 50,  43,  43,  80, 161,  62,  97, 306,  52, 106],\n",
        " [150,  67,  16,  32,  44,  34,  52,  41, 482,  82],\n",
        " [ 66, 132,  11,  45,  40,  19,  71,  55, 103, 458]]\n",
        "\n",
        "a=np.array(a)\n",
        "a\n",
        "names=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "df2=pd.DataFrame(a, columns=names, index=names)\n",
        "df2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airplane</th>\n",
              "      <th>automobile</th>\n",
              "      <th>bird</th>\n",
              "      <th>cat</th>\n",
              "      <th>deer</th>\n",
              "      <th>dog</th>\n",
              "      <th>frog</th>\n",
              "      <th>horse</th>\n",
              "      <th>ship</th>\n",
              "      <th>truck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>airplane</th>\n",
              "      <td>459</td>\n",
              "      <td>40</td>\n",
              "      <td>47</td>\n",
              "      <td>42</td>\n",
              "      <td>73</td>\n",
              "      <td>20</td>\n",
              "      <td>64</td>\n",
              "      <td>37</td>\n",
              "      <td>160</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>automobile</th>\n",
              "      <td>60</td>\n",
              "      <td>352</td>\n",
              "      <td>26</td>\n",
              "      <td>57</td>\n",
              "      <td>41</td>\n",
              "      <td>48</td>\n",
              "      <td>83</td>\n",
              "      <td>40</td>\n",
              "      <td>101</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bird</th>\n",
              "      <td>99</td>\n",
              "      <td>42</td>\n",
              "      <td>122</td>\n",
              "      <td>90</td>\n",
              "      <td>250</td>\n",
              "      <td>56</td>\n",
              "      <td>215</td>\n",
              "      <td>63</td>\n",
              "      <td>27</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>54</td>\n",
              "      <td>47</td>\n",
              "      <td>67</td>\n",
              "      <td>255</td>\n",
              "      <td>88</td>\n",
              "      <td>133</td>\n",
              "      <td>187</td>\n",
              "      <td>68</td>\n",
              "      <td>36</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deer</th>\n",
              "      <td>48</td>\n",
              "      <td>21</td>\n",
              "      <td>63</td>\n",
              "      <td>50</td>\n",
              "      <td>422</td>\n",
              "      <td>28</td>\n",
              "      <td>238</td>\n",
              "      <td>79</td>\n",
              "      <td>29</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>40</td>\n",
              "      <td>24</td>\n",
              "      <td>74</td>\n",
              "      <td>173</td>\n",
              "      <td>87</td>\n",
              "      <td>288</td>\n",
              "      <td>150</td>\n",
              "      <td>86</td>\n",
              "      <td>31</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>frog</th>\n",
              "      <td>21</td>\n",
              "      <td>38</td>\n",
              "      <td>50</td>\n",
              "      <td>46</td>\n",
              "      <td>150</td>\n",
              "      <td>42</td>\n",
              "      <td>565</td>\n",
              "      <td>38</td>\n",
              "      <td>10</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>horse</th>\n",
              "      <td>50</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>80</td>\n",
              "      <td>161</td>\n",
              "      <td>62</td>\n",
              "      <td>97</td>\n",
              "      <td>306</td>\n",
              "      <td>52</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ship</th>\n",
              "      <td>150</td>\n",
              "      <td>67</td>\n",
              "      <td>16</td>\n",
              "      <td>32</td>\n",
              "      <td>44</td>\n",
              "      <td>34</td>\n",
              "      <td>52</td>\n",
              "      <td>41</td>\n",
              "      <td>482</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>truck</th>\n",
              "      <td>66</td>\n",
              "      <td>132</td>\n",
              "      <td>11</td>\n",
              "      <td>45</td>\n",
              "      <td>40</td>\n",
              "      <td>19</td>\n",
              "      <td>71</td>\n",
              "      <td>55</td>\n",
              "      <td>103</td>\n",
              "      <td>458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            airplane  automobile  bird  cat  ...  frog  horse  ship  truck\n",
              "airplane         459          40    47   42  ...    64     37   160     58\n",
              "automobile        60         352    26   57  ...    83     40   101    192\n",
              "bird              99          42   122   90  ...   215     63    27     36\n",
              "cat               54          47    67  255  ...   187     68    36     65\n",
              "deer              48          21    63   50  ...   238     79    29     22\n",
              "dog               40          24    74  173  ...   150     86    31     47\n",
              "frog              21          38    50   46  ...   565     38    10     40\n",
              "horse             50          43    43   80  ...    97    306    52    106\n",
              "ship             150          67    16   32  ...    52     41   482     82\n",
              "truck             66         132    11   45  ...    71     55   103    458\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwcCFkDL2haI",
        "colab_type": "code",
        "outputId": "636d7734-335d-46b1-d20b-5aa16a93ffdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "sns.heatmap(-df2/df2.sum(axis=1)+0.3,)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff3da0bbeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEmCAYAAABFx2beAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEchvOIR60LewAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsv5GiTeRFi8",
        "colab_type": "code",
        "outputId": "e2bf3762-aad0-4696-b1f5-69b7588d5419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "conf/conf.sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.865, 0.01 , 0.026, 0.009, 0.018, 0.003, 0.005, 0.014, 0.02 ,\n",
              "        0.03 ],\n",
              "       [0.016, 0.892, 0.002, 0.003, 0.   , 0.   , 0.002, 0.001, 0.011,\n",
              "        0.073],\n",
              "       [0.058, 0.002, 0.671, 0.018, 0.1  , 0.037, 0.066, 0.025, 0.006,\n",
              "        0.017],\n",
              "       [0.023, 0.008, 0.059, 0.508, 0.103, 0.109, 0.108, 0.045, 0.01 ,\n",
              "        0.027],\n",
              "       [0.01 , 0.002, 0.027, 0.009, 0.844, 0.013, 0.049, 0.042, 0.003,\n",
              "        0.001],\n",
              "       [0.007, 0.006, 0.038, 0.074, 0.053, 0.695, 0.046, 0.062, 0.002,\n",
              "        0.017],\n",
              "       [0.005, 0.001, 0.018, 0.013, 0.028, 0.007, 0.917, 0.003, 0.004,\n",
              "        0.004],\n",
              "       [0.006, 0.   , 0.016, 0.016, 0.034, 0.023, 0.006, 0.885, 0.003,\n",
              "        0.011],\n",
              "       [0.077, 0.018, 0.008, 0.003, 0.004, 0.005, 0.002, 0.004, 0.848,\n",
              "        0.031],\n",
              "       [0.028, 0.029, 0.003, 0.001, 0.001, 0.   , 0.002, 0.007, 0.009,\n",
              "        0.92 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3_GdCFqTRY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('cnnmodel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDAGl-MnVKzQ",
        "colab_type": "code",
        "outputId": "f126cae9-2455-4496-c788-433b60ee987c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "model.get_layer(name='conv1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.convolutional.Conv2D at 0x7fa837b62208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}